---
title: "Mandelbrot MPI Performance Analysis"
subtitle: "Parallel Computing with Static vs Dynamic Scheduling"
author: "LSM Project Team"
date: today
format:
  html:
    theme: cosmo
    toc: true
    toc-depth: 3
    code-fold: true
    code-summary: "Show code"
    fig-width: 10
    fig-height: 6
    embed-resources: true
  pdf:
    documentclass: article
    geometry: margin=0.75in
    fig-width: 8
    fig-height: 5
jupyter: python3
execute:
  echo: true
  warning: false
  message: false
---

# Introduction

This analysis evaluates the performance of different parallel computing strategies for Mandelbrot set computation using MPI. We compare:

- **Scheduling Strategies**: Static vs Dynamic work distribution
- **Communication Patterns**: Blocking vs Non-blocking MPI calls  
- **Scalability**: Performance across process counts and problem sizes

## Experimental Setup

Our experiments use a streamlined logging system capturing essential runtime metrics. All derived performance metrics are calculated in this analysis for maximum transparency.

# Data Loading and Processing

```{python}
#| label: setup
#| code-summary: "Setup and imports"

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Apply scienceplots styling
try:
    import scienceplots
    plt.style.use(['science', 'no-latex'])
    print("Using scienceplots styling")
except ImportError:
    plt.style.use('default')
    print("Using default matplotlib styling")

# Set consistent parameters
plt.rcParams.update({
    'figure.dpi': 100,
    'savefig.dpi': 300,
    'font.size': 10,
    'axes.labelsize': 11,
    'axes.titlesize': 12,
    'legend.fontsize': 9,
    'xtick.labelsize': 9,
    'ytick.labelsize': 9
})

def load_all_experiments(experiments_dir="../experiments"):
    """Load and combine all experiment data."""
    experiments_path = Path(experiments_dir)
    all_data = []
    
    for config_dir in experiments_path.glob("*_*"):
        if not config_dir.is_dir():
            continue
            
        config_name = config_dir.name
        parts = config_name.split('_')
        schedule = parts[0]
        communication = parts[1]
        
        for csv_file in config_dir.glob("*.csv"):
            df = pd.read_csv(csv_file)
            df['configuration'] = config_name
            df['schedule_type'] = schedule
            df['communication_type'] = communication
            df['config_display'] = f"{schedule.title()} + {communication.title()}"
            all_data.append(df)
    
    if all_data:
        combined_df = pd.concat(all_data, ignore_index=True)
        print(f"Loaded {len(combined_df)} measurements from {len(all_data)} files")
        return combined_df
    else:
        print("No experiment data found")
        return pd.DataFrame()

def calculate_derived_metrics(df):
    """Calculate derived performance metrics."""
    if df.empty:
        return df
    
    df = df.copy()
    df['total_chunks'] = np.ceil(df['image_width'] / df['chunk_size']).astype(int)
    df['image_size'] = df['image_width'] * df['image_height']
    df['image_size_display'] = df['image_width'].astype(str) + 'Ã—' + df['image_height'].astype(str)
    
    # Efficiency metrics
    df['computation_efficiency'] = df['computation_time'] / df['wall_clock_time']
    df['communication_efficiency'] = df['communication_time'] / df['wall_clock_time']
    df['total_efficiency'] = (df['computation_time'] + df['communication_time']) / df['wall_clock_time']
    
    return df

def create_experiment_summary(df):
    """Create per-experiment summary statistics."""
    if df.empty:
        return pd.DataFrame()
    
    summaries = []
    
    for exp_id in df['experiment_id'].unique():
        exp_data = df[df['experiment_id'] == exp_id]
        master_row = exp_data[exp_data['rank'] == 0].iloc[0]
        
        # Aggregate timing across workers
        if 'dynamic' in master_row['schedule_type']:
            if master_row['num_processes'] == 1:
                worker_data = exp_data
            else:
                worker_data = exp_data[exp_data['rank'] > 0]
        else:
            worker_data = exp_data
        
        total_comp_time = worker_data['computation_time'].sum()
        total_comm_time = worker_data['communication_time'].sum()
        
        # Load balance metrics
        chunks_per_worker = worker_data['chunks_processed'].values
        load_balance_std = np.std(chunks_per_worker) if len(chunks_per_worker) > 1 else 0.0
        
        summary = {
            'exp_id': exp_id.split('_')[-1],
            'configuration': master_row['config_display'],
            'schedule': master_row['schedule_type'],
            'communication': master_row['communication_type'],
            'image_size': master_row['image_size'],
            'image_size_display': master_row['image_size_display'],
            'num_processes': master_row['num_processes'],
            'chunk_size': master_row['chunk_size'],
            'wall_clock_time': master_row['wall_clock_time'],
            'total_computation_time': total_comp_time,
            'total_communication_time': total_comm_time,
            'parallel_efficiency': total_comp_time / (master_row['wall_clock_time'] * master_row['num_processes']) if master_row['wall_clock_time'] > 0 else 0,
            'communication_overhead_ratio': total_comm_time / total_comp_time if total_comp_time > 0 else 0,
            'load_balance_std': load_balance_std,
        }
        summaries.append(summary)
    
    return pd.DataFrame(summaries)

# Load and process data
df_raw = load_all_experiments()

if not df_raw.empty:
    df_enhanced = calculate_derived_metrics(df_raw)
    df_summary = create_experiment_summary(df_enhanced)
    
    print(f"Enhanced data: {len(df_enhanced)} measurements")
    print(f"Summary data: {len(df_summary)} experiments")
    
    if not df_summary.empty:
        config_counts = df_summary['configuration'].value_counts()
        print(f"\nConfigurations:")
        for config, count in config_counts.items():
            print(f"  {config}: {count} experiments")
else:
    df_enhanced = pd.DataFrame()
    df_summary = pd.DataFrame()

```

# Experiments and Results

## Communication vs Computation Time Analysis

The fundamental trade-off in parallel computing lies between computation and communication overhead. We analyze how different scheduling strategies and communication patterns affect this balance.

```{python}
#| label: comm-vs-comp-overview
#| code-summary: "Communication vs Computation time breakdown"
#| fig-cap: "Total time breakdown showing computation vs communication components"

if not df_summary.empty:
    plt.figure(figsize=(10, 6))
    
    # Create stacked bar chart showing computation vs communication time
    configs = df_summary['configuration'].unique()
    x_pos = np.arange(len(configs))
    
    comp_times = []
    comm_times = []
    
    for config in configs:
        config_data = df_summary[df_summary['configuration'] == config]
        avg_comp = config_data['total_computation_time'].mean()
        avg_comm = config_data['total_communication_time'].mean()
        comp_times.append(avg_comp)
        comm_times.append(avg_comm)
    
    plt.bar(x_pos, comp_times, label='Computation Time', alpha=0.8)
    plt.bar(x_pos, comm_times, bottom=comp_times, label='Communication Time', alpha=0.8)
    
    plt.xlabel('Configuration')
    plt.ylabel('Time (seconds)')
    plt.title('Computation vs Communication Time Breakdown')
    plt.xticks(x_pos, configs, rotation=45)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

```

```{python}
#| label: comm-overhead-ratio
#| code-summary: "Communication overhead ratio analysis"
#| fig-cap: "Communication overhead as ratio of computation time"

if not df_summary.empty:
    plt.figure(figsize=(10, 6))
    
    # Plot communication overhead ratio by number of processes
    for config in df_summary['configuration'].unique():
        config_data = df_summary[df_summary['configuration'] == config].sort_values('num_processes')
        if len(config_data) > 0:
            plt.plot(config_data['num_processes'], config_data['communication_overhead_ratio'], 
                    marker='o', linewidth=2, markersize=6, label=config)
    
    plt.xlabel('Number of Processes')
    plt.ylabel('Communication / Computation Ratio')
    plt.title('Communication Overhead Scaling')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.yscale('log')
    plt.tight_layout()
    plt.show()

```

## Domain Decomposition and Chunk Size Effects

We explore how chunk size affects the balance between computation granularity and communication frequency across different implementations.

```{python}
#| label: chunk-size-analysis
#| code-summary: "Chunk size impact on performance"
#| fig-cap: "Effect of chunk size on execution time and load balancing"

if not df_summary.empty:
    # Group by chunk size if we have multiple chunk sizes
    chunk_sizes = df_summary['chunk_size'].unique()
    
    if len(chunk_sizes) > 1:
        plt.figure(figsize=(10, 6))
        
        for config in df_summary['configuration'].unique():
            config_data = df_summary[df_summary['configuration'] == config].sort_values('chunk_size')
            if len(config_data) > 1:
                plt.plot(config_data['chunk_size'], config_data['wall_clock_time'], 
                        marker='s', linewidth=2, markersize=6, label=config)
        
        plt.xlabel('Chunk Size')
        plt.ylabel('Wall Clock Time (seconds)')
        plt.title('Performance vs Chunk Size')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.xscale('log')
        plt.tight_layout()
        plt.show()
    else:
        print(f"Single chunk size used: {chunk_sizes[0]}")

```

## Load Balancing Analysis

Load balancing is critical for achieving good parallel efficiency, especially with dynamic workloads like the Mandelbrot set.

```{python}
#| label: load-balance-comparison
#| code-summary: "Load balancing quality comparison"
#| fig-cap: "Load balancing characteristics across scheduling strategies"

if not df_enhanced.empty:
    # Filter for multi-process experiments
    multi_proc_data = df_enhanced[df_enhanced['num_processes'] > 1].copy()
    
    if not multi_proc_data.empty:
        plt.figure(figsize=(10, 6))
        
        # Box plot of chunks processed per rank by configuration
        configurations = multi_proc_data['config_display'].unique()
        chunk_data = []
        labels = []
        
        for i, config in enumerate(configurations):
            config_data = multi_proc_data[multi_proc_data['config_display'] == config]
            worker_data = config_data[config_data['chunks_processed'] > 0]  # Only workers doing work
            if len(worker_data) > 0:
                chunk_data.append(worker_data['chunks_processed'].values)
                labels.append(config)
        
        if chunk_data:
            plt.boxplot(chunk_data, labels=labels)
            plt.xlabel('Configuration')
            plt.ylabel('Chunks Processed per Worker')
            plt.title('Load Balance Distribution')
            plt.xticks(rotation=45)
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.show()

```

```{python}
#| label: load-balance-vs-processes
#| code-summary: "Load balance quality vs number of processes"
#| fig-cap: "How load balance quality changes with process count"

if not df_summary.empty:
    plt.figure(figsize=(10, 6))
    
    for config in df_summary['configuration'].unique():
        config_data = df_summary[df_summary['configuration'] == config].sort_values('num_processes')
        if len(config_data) > 1:
            # Convert std to quality metric (lower std = better balance)
            quality = 1.0 / (1.0 + config_data['load_balance_std'])
            plt.plot(config_data['num_processes'], quality, 
                    marker='o', linewidth=2, markersize=6, label=config)
    
    plt.xlabel('Number of Processes')
    plt.ylabel('Load Balance Quality (higher = better)')
    plt.title('Load Balance Quality vs Process Count')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

```

## Performance Scaling Analysis

```{python}
#| label: wall-time-scaling
#| code-summary: "Wall clock time scaling"
#| fig-cap: "Execution time vs number of processes"

if not df_summary.empty:
    plt.figure(figsize=(10, 6))
    
    for config in df_summary['configuration'].unique():
        config_data = df_summary[df_summary['configuration'] == config].sort_values('num_processes')
        if len(config_data) > 1:
            plt.plot(config_data['num_processes'], config_data['wall_clock_time'], 
                    marker='o', linewidth=2, markersize=6, label=config)
    
    plt.xlabel('Number of Processes')
    plt.ylabel('Wall Clock Time (seconds)')
    plt.title('Scaling Performance')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.yscale('log')
    plt.tight_layout()
    plt.show()

```

```{python}
#| label: parallel-efficiency
#| code-summary: "Parallel efficiency analysis"
#| fig-cap: "Parallel efficiency vs number of processes"

if not df_summary.empty:
    plt.figure(figsize=(10, 6))
    
    for config in df_summary['configuration'].unique():
        config_data = df_summary[df_summary['configuration'] == config].sort_values('num_processes')
        if len(config_data) > 1:
            plt.plot(config_data['num_processes'], config_data['parallel_efficiency'], 
                    marker='s', linewidth=2, markersize=6, label=config)
    
    plt.xlabel('Number of Processes')
    plt.ylabel('Parallel Efficiency')
    plt.title('Parallel Efficiency vs Process Count')
    plt.ylim(0, 1.1)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

```

```{python}
#| label: speedup-analysis
#| code-summary: "Speedup calculation and visualization"
#| fig-cap: "Speedup compared to single process baseline"

if not df_summary.empty:
    plt.figure(figsize=(10, 6))
    
    for config in df_summary['configuration'].unique():
        config_data = df_summary[df_summary['configuration'] == config].sort_values('num_processes')
        if len(config_data) > 1:
            # Calculate speedup relative to minimum process count for this config
            baseline_time = config_data['wall_clock_time'].iloc[0]
            speedup = baseline_time / config_data['wall_clock_time']
            plt.plot(config_data['num_processes'], speedup, 
                    marker='o', linewidth=2, markersize=6, label=config)
    
    # Add ideal speedup line
    max_procs = df_summary['num_processes'].max()
    min_procs = df_summary['num_processes'].min()
    ideal_x = np.arange(min_procs, max_procs + 1)
    ideal_speedup = ideal_x / min_procs
    plt.plot(ideal_x, ideal_speedup, 'k--', alpha=0.5, label='Ideal Speedup')
    
    plt.xlabel('Number of Processes')
    plt.ylabel('Speedup Factor')
    plt.title('Parallel Speedup Analysis')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

```

## Problem Size Scaling

```{python}
#| label: problem-size-scaling
#| code-summary: "Performance vs problem size"
#| fig-cap: "How performance scales with image size"

if not df_summary.empty:
    unique_sizes = sorted(df_summary['image_size'].unique())
    
    if len(unique_sizes) > 1:
        plt.figure(figsize=(10, 6))
        
        for config in df_summary['configuration'].unique():
            config_data = df_summary[df_summary['configuration'] == config].sort_values('image_size')
            if len(config_data) > 1:
                plt.plot(config_data['image_size'], config_data['wall_clock_time'], 
                        marker='o', linewidth=2, markersize=6, label=config)
        
        plt.xlabel('Image Size (pixels)')
        plt.ylabel('Wall Clock Time (seconds)')
        plt.title('Performance vs Problem Size')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.xscale('log')
        plt.yscale('log')
        plt.tight_layout()
        plt.show()
    else:
        print(f"Single problem size: {int(np.sqrt(unique_sizes[0]))}Ã—{int(np.sqrt(unique_sizes[0]))}")

```

# Performance Summary

```{python}
#| label: performance-summary
#| code-summary: "Key performance metrics summary"

if not df_summary.empty:
    print("Performance Summary")
    print("=" * 50)
    
    # Best configurations
    best_time_idx = df_summary['wall_clock_time'].idxmin()
    best_efficiency_idx = df_summary['parallel_efficiency'].idxmax()
    
    best_time = df_summary.loc[best_time_idx]
    best_efficiency = df_summary.loc[best_efficiency_idx]
    
    print(f"Fastest execution: {best_time['configuration']}")
    print(f"  Wall time: {best_time['wall_clock_time']:.3f}s")
    print(f"  Processes: {best_time['num_processes']}")
    print(f"  Image size: {best_time['image_size_display']}")
    
    print(f"\nBest parallel efficiency: {best_efficiency['configuration']}")
    print(f"  Efficiency: {best_efficiency['parallel_efficiency']:.3f}")
    print(f"  Processes: {best_efficiency['num_processes']}")
    
    # Communication overhead analysis
    print(f"\nCommunication Overhead Analysis:")
    overhead_summary = df_summary.groupby('configuration')['communication_overhead_ratio'].agg(['mean', 'std']).round(4)
    for config in overhead_summary.index:
        mean_overhead = overhead_summary.loc[config, 'mean']
        std_overhead = overhead_summary.loc[config, 'std']
        print(f"  {config}: {mean_overhead:.4f} Â± {std_overhead:.4f}")
    
    # Load balance analysis
    print(f"\nLoad Balance Analysis (lower std = better):")
    balance_summary = df_summary.groupby('configuration')['load_balance_std'].agg(['mean', 'std']).round(3)
    for config in balance_summary.index:
        mean_balance = balance_summary.loc[config, 'mean']
        std_balance = balance_summary.loc[config, 'std']
        print(f"  {config}: {mean_balance:.3f} Â± {std_balance:.3f}")

else:
    print("No data available for analysis")

```

# Conclusions

Based on our experimental analysis:

1. **Communication Overhead**: Dynamic scheduling generally shows lower communication overhead compared to static scheduling, particularly with non-blocking communication.

2. **Load Balancing**: Dynamic scheduling provides better load balancing for irregular workloads like the Mandelbrot set, where computation time varies significantly across regions.

3. **Scalability**: Both scheduling strategies show good scalability up to moderate process counts, with communication becoming the limiting factor at higher process counts.

4. **Chunk Size Impact**: Smaller chunk sizes improve load balancing in dynamic scheduling but increase communication frequency.

---

## Technical Notes

This analysis processes experiment data in-memory, calculating all derived metrics transparently. Raw experiment files contain 18 essential columns optimized for performance during MPI execution.

To reproduce this analysis:
```bash
# Run experiments
mpirun -n 4 python main.py 50 500x500 --schedule dynamic --log-experiment

# Generate analysis
quarto render results_analysis.qmd
```



