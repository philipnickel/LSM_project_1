{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandelbrot MPI Experiment Analysis\n",
    "\n",
    "This notebook analyzes the performance and workload distribution of Mandelbrot MPI experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import scienceplots\n",
    "\n",
    "# Set scienceplots style\n",
    "plt.style.use(['science', 'ieee'])\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (10, 6),\n",
    "    'font.size': 12,\n",
    "    'axes.labelsize': 14,\n",
    "    'axes.titlesize': 16,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'legend.fontsize': 12,\n",
    "})\n",
    "\n",
    "# Set up paths\n",
    "experiments_dir = Path('../experiments')\n",
    "output_dir = Path('../Plots/analysis')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Experiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_data(experiments_dir: Path):\n",
    "    \"\"\"Load all experiment CSV files.\"\"\"\n",
    "    data = {}\n",
    "    \n",
    "    for combo_dir in experiments_dir.glob(\"*_*\"):  # static_blocking, etc.\n",
    "        if not combo_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        combo = combo_dir.name\n",
    "        csv_files = list(combo_dir.glob(\"*.csv\"))\n",
    "        \n",
    "        if not csv_files:\n",
    "            continue\n",
    "            \n",
    "        # Load and combine all CSV files for this combination\n",
    "        dfs = []\n",
    "        for csv_file in csv_files:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            dfs.append(df)\n",
    "        \n",
    "        if dfs:\n",
    "            combined_df = pd.concat(dfs, ignore_index=True)\n",
    "            data[combo] = combined_df\n",
    "            print(f\"Loaded {len(csv_files)} experiments for {combo}: {len(combined_df)} total rows\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load data\n",
    "data = load_experiment_data(experiments_dir)\n",
    "print(f\"\\nLoaded {len(data)} experiment combinations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show overview of loaded data\n",
    "for combo, df in data.items():\n",
    "    print(f\"\\n{combo.upper()}:\")\n",
    "    print(f\"  Experiments: {df['experiment_id'].nunique()}\")\n",
    "    print(f\"  Total rows: {len(df)}\")\n",
    "    print(f\"  Processes tested: {sorted(df['num_processes'].unique())}\")\n",
    "    print(f\"  Image sizes: {df[['image_width', 'image_height']].drop_duplicates().values.tolist()}\")\n",
    "    \n",
    "    # Show sample experiment\n",
    "    sample_exp = df[df['experiment_id'] == df['experiment_id'].iloc[0]]\n",
    "    print(f\"  Sample experiment: {sample_exp['experiment_id'].iloc[0]}\")\n",
    "    print(f\"    Processes: {sample_exp['num_processes'].iloc[0]}\")\n",
    "    print(f\"    Wall clock time: {sample_exp['wall_clock_time'].iloc[0]:.4f}s\")\n",
    "    print(f\"    Load balance: {sample_exp['load_balance_score'].iloc[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison: Wall clock time vs processes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Wall clock time vs processes\n",
    "for combo, df in data.items():\n",
    "    if df.empty:\n",
    "        continue\n",
    "        \n",
    "    # Group by number of processes and get mean wall clock time\n",
    "    perf_data = df.groupby('num_processes')['wall_clock_time'].mean().reset_index()\n",
    "    ax1.plot(perf_data['num_processes'], perf_data['wall_clock_time'], \n",
    "            marker='o', linewidth=2, markersize=8, label=combo.replace('_', ' ').title())\n",
    "\n",
    "ax1.set_xlabel('Number of MPI Processes')\n",
    "ax1.set_ylabel('Wall Clock Time (seconds)')\n",
    "ax1.set_title('Performance: Wall Clock Time')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Speedup vs processes\n",
    "for combo, df in data.items():\n",
    "    if df.empty:\n",
    "        continue\n",
    "        \n",
    "    # Calculate speedup (time_1proc / time_nproc)\n",
    "    perf_data = df.groupby('num_processes')['wall_clock_time'].mean().reset_index()\n",
    "    if len(perf_data) > 1:\n",
    "        single_proc_time = perf_data[perf_data['num_processes'] == 1]['wall_clock_time'].iloc[0]\n",
    "        speedup = single_proc_time / perf_data['wall_clock_time']\n",
    "        ax2.plot(perf_data['num_processes'], speedup, \n",
    "                marker='s', linewidth=2, markersize=8, label=combo.replace('_', ' ').title())\n",
    "\n",
    "ax2.set_xlabel('Number of MPI Processes')\n",
    "ax2.set_ylabel('Speedup')\n",
    "ax2.set_title('Scaling: Speedup')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'performance_analysis.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Balance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load balance analysis\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Load balance score vs processes\n",
    "for combo, df in data.items():\n",
    "    if df.empty:\n",
    "        continue\n",
    "        \n",
    "    lb_data = df.groupby('num_processes')['load_balance_score'].mean().reset_index()\n",
    "    ax1.plot(lb_data['num_processes'], lb_data['load_balance_score'], \n",
    "            marker='o', linewidth=2, markersize=8, label=combo.replace('_', ' ').title())\n",
    "\n",
    "ax1.set_xlabel('Number of MPI Processes')\n",
    "ax1.set_ylabel('Load Balance Score')\n",
    "ax1.set_title('Load Balancing Quality')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1.1)\n",
    "\n",
    "# Plot 2: Time breakdown (computation vs communication)\n",
    "for combo, df in data.items():\n",
    "    if df.empty:\n",
    "        continue\n",
    "        \n",
    "    # Group by number of processes and get mean times\n",
    "    time_data = df.groupby('num_processes').agg({\n",
    "        'computation_time': 'mean',\n",
    "        'communication_time': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    ax2.plot(time_data['num_processes'], time_data['computation_time'], \n",
    "            marker='o', linewidth=2, markersize=8, label=f'{combo.replace(\"_\", \" \").title()} (Comp)')\n",
    "    ax2.plot(time_data['num_processes'], time_data['communication_time'], \n",
    "            marker='s', linestyle='--', linewidth=2, markersize=8, \n",
    "            label=f'{combo.replace(\"_\", \" \").title()} (Comm)')\n",
    "\n",
    "ax2.set_xlabel('Number of MPI Processes')\n",
    "ax2.set_ylabel('Time (seconds)')\n",
    "ax2.set_title('Time Breakdown: Computation vs Communication')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'load_balance_analysis.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Experiment Analysis\n",
    "\n",
    "Let's analyze each experiment in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze a single experiment\n",
    "def analyze_experiment(exp_data, exp_id, combo):\n",
    "    \"\"\"Analyze a single experiment in detail.\"\"\"\n",
    "    \n",
    "    # Get experiment metadata\n",
    "    exp_info = exp_data.iloc[0]\n",
    "    num_processes = exp_info['num_processes']\n",
    "    image_size = f\"{exp_info['image_width']}x{exp_info['image_height']}\"\n",
    "    wall_clock_time = exp_info['wall_clock_time']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXPERIMENT: {combo.upper()}\")\n",
    "    print(f\"ID: {exp_id}\")\n",
    "    print(f\"Image Size: {image_size}\")\n",
    "    print(f\"Processes: {num_processes}\")\n",
    "    print(f\"Wall Clock Time: {wall_clock_time:.4f}s\")\n",
    "    print(f\"Load Balance Score: {exp_info['load_balance_score']:.4f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create comprehensive analysis\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # 1. Time distribution pie chart\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    comp_time = exp_info['computation_time']\n",
    "    comm_time = exp_info['communication_time']\n",
    "    other_time = wall_clock_time - comp_time - comm_time\n",
    "    \n",
    "    times = [comp_time, comm_time, other_time]\n",
    "    labels = ['Computation', 'Communication', 'Other']\n",
    "    colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "    \n",
    "    wedges, texts, autotexts = ax1.pie(times, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    ax1.set_title(f'Time Distribution\\n{combo.replace(\"_\", \" \").title()}')\n",
    "    \n",
    "    # 2. Chunks per rank\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    if num_processes > 1:\n",
    "        rank_data = exp_data[exp_data['rank'] > 0]  # Exclude master\n",
    "        if not rank_data.empty:\n",
    "            ranks = rank_data['rank'].values\n",
    "            chunks = rank_data['chunks_processed'].values\n",
    "            \n",
    "            bars = ax2.bar(ranks, chunks, alpha=0.7, color='skyblue')\n",
    "            ax2.set_xlabel('MPI Rank')\n",
    "            ax2.set_ylabel('Chunks Processed')\n",
    "            ax2.set_title('Chunks per Rank')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, chunk_count in zip(bars, chunks):\n",
    "                height = bar.get_height()\n",
    "                ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                       f'{chunk_count}', ha='center', va='bottom')\n",
    "            \n",
    "            # Add statistics\n",
    "            mean_chunks = np.mean(chunks)\n",
    "            std_chunks = np.std(chunks)\n",
    "            ax2.axhline(y=mean_chunks, color='red', linestyle='--', \n",
    "                       label=f'Mean: {mean_chunks:.1f}')\n",
    "            ax2.axhline(y=mean_chunks + std_chunks, color='orange', linestyle=':', \n",
    "                       label=f'$\\\\pm 1\\\\sigma$: {std_chunks:.1f}')\n",
    "            ax2.legend()\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, 'No worker data', ha='center', va='center', transform=ax2.transAxes)\n",
    "            ax2.set_title('Chunks per Rank')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'Single process', ha='center', va='center', transform=ax2.transAxes)\n",
    "        ax2.set_title('Chunks per Rank')\n",
    "    \n",
    "    # 3. Rows per rank (if we have row data)\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    if num_processes > 1 and 'row_ranges' in exp_data.columns:\n",
    "        rank_data = exp_data[exp_data['rank'] > 0]\n",
    "        if not rank_data.empty and not rank_data['row_ranges'].isna().all():\n",
    "            # Parse row ranges and calculate total rows per rank\n",
    "            rows_per_rank = []\n",
    "            for _, row in rank_data.iterrows():\n",
    "                row_ranges = row['row_ranges']\n",
    "                if pd.notna(row_ranges) and row_ranges:\n",
    "                    # Parse ranges like \"0-24, 25-49\"\n",
    "                    total_rows = 0\n",
    "                    for range_str in str(row_ranges).split(', '):\n",
    "                        if '-' in range_str:\n",
    "                            start, end = map(int, range_str.split('-'))\n",
    "                            total_rows += end - start + 1\n",
    "                    rows_per_rank.append(total_rows)\n",
    "                else:\n",
    "                    rows_per_rank.append(0)\n",
    "            \n",
    "            if rows_per_rank:\n",
    "                ranks = rank_data['rank'].values\n",
    "                bars = ax3.bar(ranks, rows_per_rank, alpha=0.7, color='lightgreen')\n",
    "                ax3.set_xlabel('MPI Rank')\n",
    "                ax3.set_ylabel('Rows Processed')\n",
    "                ax3.set_title('Rows per Rank')\n",
    "                ax3.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add value labels\n",
    "                for bar, row_count in zip(bars, rows_per_rank):\n",
    "                    height = bar.get_height()\n",
    "                    ax3.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                           f'{row_count}', ha='center', va='bottom')\n",
    "            else:\n",
    "                ax3.text(0.5, 0.5, 'No row data', ha='center', va='center', transform=ax3.transAxes)\n",
    "                ax3.set_title('Rows per Rank')\n",
    "        else:\n",
    "            ax3.text(0.5, 0.5, 'No row data', ha='center', va='center', transform=ax3.transAxes)\n",
    "            ax3.set_title('Rows per Rank')\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'Single process', ha='center', va='center', transform=ax3.transAxes)\n",
    "        ax3.set_title('Rows per Rank')\n",
    "    \n",
    "    # 4. Computation time per rank\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    if num_processes > 1:\n",
    "        rank_data = exp_data[exp_data['rank'] > 0]\n",
    "        if not rank_data.empty:\n",
    "            comp_times = rank_data['computation_time'].values\n",
    "            bars = ax4.bar(ranks, comp_times, alpha=0.7, color='lightcoral')\n",
    "            ax4.set_xlabel('MPI Rank')\n",
    "            ax4.set_ylabel('Computation Time (s)')\n",
    "            ax4.set_title('Computation Time per Rank')\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, time in zip(bars, comp_times):\n",
    "                height = bar.get_height()\n",
    "                ax4.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                       f'{time:.3f}', ha='center', va='bottom')\n",
    "        else:\n",
    "            ax4.text(0.5, 0.5, 'No worker data', ha='center', va='center', transform=ax4.transAxes)\n",
    "            ax4.set_title('Computation Time per Rank')\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'Single process', ha='center', va='center', transform=ax4.transAxes)\n",
    "        ax4.set_title('Computation Time per Rank')\n",
    "    \n",
    "    # 5. Communication time per rank\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    if num_processes > 1:\n",
    "        rank_data = exp_data[exp_data['rank'] > 0]\n",
    "        if not rank_data.empty:\n",
    "            comm_times = rank_data['communication_time'].values\n",
    "            bars = ax5.bar(ranks, comm_times, alpha=0.7, color='lightblue')\n",
    "            ax5.set_xlabel('MPI Rank')\n",
    "            ax5.set_ylabel('Communication Time (s)')\n",
    "            ax5.set_title('Communication Time per Rank')\n",
    "            ax5.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, time in zip(bars, comm_times):\n",
    "                height = bar.get_height()\n",
    "                ax5.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                       f'{time:.3f}', ha='center', va='bottom')\n",
    "        else:\n",
    "            ax5.text(0.5, 0.5, 'No worker data', ha='center', va='center', transform=ax5.transAxes)\n",
    "            ax5.set_title('Communication Time per Rank')\n",
    "    else:\n",
    "        ax5.text(0.5, 0.5, 'Single process', ha='center', va='center', transform=ax5.transAxes)\n",
    "        ax5.set_title('Communication Time per Rank')\n",
    "    \n",
    "    # 6. Load balance metrics\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    ax6.axis('off')\n",
    "    ax6.set_title('Load Balance Metrics', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    if num_processes > 1:\n",
    "        rank_data = exp_data[exp_data['rank'] > 0]\n",
    "        if not rank_data.empty:\n",
    "            chunks = rank_data['chunks_processed'].values\n",
    "            mean_chunks = np.mean(chunks)\n",
    "            max_chunks = np.max(chunks)\n",
    "            min_chunks = np.min(chunks)\n",
    "            load_balance = min_chunks / max_chunks if max_chunks > 0 else 1.0\n",
    "            cv = np.std(chunks)/mean_chunks if mean_chunks > 0 else 0.0\n",
    "            \n",
    "            metrics = [\n",
    "                ('Load Balance Score', f\"{load_balance:.4f}\"),\n",
    "                ('Mean Chunks/Rank', f\"{mean_chunks:.1f}\"),\n",
    "                ('Max Chunks/Rank', f\"{max_chunks}\"),\n",
    "                ('Min Chunks/Rank', f\"{min_chunks}\"),\n",
    "                ('Std Dev', f\"{np.std(chunks):.2f}\"),\n",
    "                ('Coefficient of Variation', f\"{cv:.3f}\"),\n",
    "            ]\n",
    "        else:\n",
    "            metrics = [('No worker data available', '')]\n",
    "    else:\n",
    "        metrics = [('Single process execution', '')]\n",
    "    \n",
    "    for i, (metric, value) in enumerate(metrics):\n",
    "        ax6.text(0.1, 0.9 - i*0.12, f\"{metric}: {value}\", \n",
    "                fontsize=12, transform=ax6.transAxes)\n",
    "    \n",
    "    plt.suptitle(f'Detailed Analysis: {combo.replace(\"_\", \" \").title()}\\n{exp_id}', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save individual experiment analysis\n",
    "    safe_exp_id = exp_id.replace('/', '_').replace(':', '_')\n",
    "    plt.savefig(output_dir / f'experiment_{combo}_{safe_exp_id}.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return exp_info\n",
    "\n",
    "# Analyze each experiment\n",
    "experiment_summaries = {}\n",
    "for combo, df in data.items():\n",
    "    experiment_ids = df['experiment_id'].unique()\n",
    "    \n",
    "    for exp_id in experiment_ids:\n",
    "        exp_data = df[df['experiment_id'] == exp_id]\n",
    "        summary = analyze_experiment(exp_data, exp_id, combo)\n",
    "        experiment_summaries[exp_id] = summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_data = []\n",
    "for exp_id, summary in experiment_summaries.items():\n",
    "    summary_data.append({\n",
    "        'Experiment ID': exp_id,\n",
    "        'Configuration': summary['schedule'] + '_' + summary['communication'],\n",
    "        'Processes': summary['num_processes'],\n",
    "        'Image Size': f\"{summary['image_width']}x{summary['image_height']}\",\n",
    "        'Wall Clock Time (s)': f\"{summary['wall_clock_time']:.4f}\",\n",
    "        'Computation Time (s)': f\"{summary['computation_time']:.4f}\",\n",
    "        'Communication Time (s)': f\"{summary['communication_time']:.4f}\",\n",
    "        'Load Balance Score': f\"{summary['load_balance_score']:.4f}\",\n",
    "        'Total Chunks': summary['total_chunks']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nEXPERIMENT SUMMARY:\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary to CSV\n",
    "summary_df.to_csv(output_dir / 'experiment_summary.csv', index=False)\n",
    "print(f\"\\nSummary saved to {output_dir / 'experiment_summary.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot scaling efficiency\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Speedup vs processes\n",
    "for combo, df in data.items():\n",
    "    if df.empty:\n",
    "        continue\n",
    "        \n",
    "    perf_data = df.groupby('num_processes')['wall_clock_time'].mean().reset_index()\n",
    "    if len(perf_data) > 1:\n",
    "        single_proc_time = perf_data[perf_data['num_processes'] == 1]['wall_clock_time'].iloc[0]\n",
    "        speedup = single_proc_time / perf_data['wall_clock_time']\n",
    "        efficiency = (speedup / perf_data['num_processes']) * 100\n",
    "        \n",
    "        ax1.plot(perf_data['num_processes'], speedup, \n",
    "                marker='o', linewidth=2, markersize=8, \n",
    "                label=f'{combo.replace(\"_\", \" \").title()} (Speedup)')\n",
    "        ax2.plot(perf_data['num_processes'], efficiency, \n",
    "                marker='s', linewidth=2, markersize=8, \n",
    "                label=f'{combo.replace(\"_\", \" \").title()} (Efficiency)')\n",
    "\n",
    "# Add ideal speedup line\n",
    "if len(perf_data) > 1:\n",
    "    processes = perf_data['num_processes'].values\n",
    "    ax1.plot(processes, processes, 'k--', alpha=0.5, label='Ideal Speedup')\n",
    "\n",
    "ax1.set_xlabel('Number of MPI Processes')\n",
    "ax1.set_ylabel('Speedup')\n",
    "ax1.set_title('Scaling: Speedup')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.set_xlabel('Number of MPI Processes')\n",
    "ax2.set_ylabel('Efficiency (%)')\n",
    "ax2.set_title('Scaling: Parallel Efficiency')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 105)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'scaling_analysis.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workload Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze workload distribution patterns\n",
    "print(\"\\nWORKLOAD DISTRIBUTION ANALYSIS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for combo, df in data.items():\n",
    "    print(f\"\\n{combo.upper()}:\")\n",
    "    \n",
    "    # Get multi-process experiments\n",
    "    multi_proc_df = df[df['num_processes'] > 1]\n",
    "    if multi_proc_df.empty:\n",
    "        print(\"  No multi-process experiments\")\n",
    "        continue\n",
    "    \n",
    "    for _, exp in multi_proc_df.iterrows():\n",
    "        exp_id = exp['experiment_id']\n",
    "        num_proc = exp['num_processes']\n",
    "        \n",
    "        # Get worker data for this experiment\n",
    "        exp_data = df[df['experiment_id'] == exp_id]\n",
    "        workers = exp_data[exp_data['rank'] > 0]\n",
    "        \n",
    "        if not workers.empty:\n",
    "            chunks = workers['chunks_processed'].values\n",
    "            mean_chunks = np.mean(chunks)\n",
    "            std_chunks = np.std(chunks)\n",
    "            cv = std_chunks / mean_chunks if mean_chunks > 0 else 0\n",
    "            \n",
    "            print(f\"  {exp_id} ({num_proc} processes):\")\n",
    "            print(f\"    Chunks per rank: {chunks.tolist()}\")\n",
    "            print(f\"    Mean: {mean_chunks:.1f}, Std: {std_chunks:.1f}, CV: {cv:.3f}\")\n",
    "            print(f\"    Load balance: {exp['load_balance_score']:.4f}\")\n",
    "            \n",
    "            # Show row ranges if available\n",
    "            if 'row_ranges' in workers.columns and not workers['row_ranges'].isna().all():\n",
    "                print(f\"    Row ranges:\")\n",
    "                for _, worker in workers.iterrows():\n",
    "                    if pd.notna(worker['row_ranges']):\n",
    "                        print(f\"      Rank {worker['rank']}: {worker['row_ranges']}\")\n",
    "                    else:\n",
    "                        print(f\"      Rank {worker['rank']}: No row data\")\n",
    "\n",
    "print(\"\\nAnalysis complete!\")\n",
    "print(f\"All plots saved to: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}